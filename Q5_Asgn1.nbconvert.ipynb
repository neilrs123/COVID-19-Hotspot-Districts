{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening JSON file \n",
    "f = open('neighbor-districts-modified.json') \n",
    "  \n",
    "# returns JSON object as a dictionary \n",
    "data = json.load(f) \n",
    "\n",
    "# sorting districts alphabetically\n",
    "districts_from_json = []\n",
    "for i in data:\n",
    "    districts_from_json.append(i)\n",
    "districts_from_json = np.array(districts_from_json)\n",
    "districts_from_json.sort()\n",
    "\n",
    "# ids for districts\n",
    "ids = np.arange(101,101+len(districts_from_json))\n",
    "\n",
    "# Dictionary for storing ids of districts\n",
    "Dict = {} \n",
    "for i in range(len(ids)):\n",
    "    Dict[districts_from_json[i]]=ids[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "district_names_from_json = []\n",
    "district_ids_from_json = []\n",
    "for i in range(len(districts_from_json)):\n",
    "    district_names_from_json.append(districts_from_json[i].split('/')[0])\n",
    "    district_ids_from_json.append(districts_from_json[i].split('/')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "districts_to_complete_ids = {}\n",
    "for i in range(len(district_names_from_json)):\n",
    "    districts_to_complete_ids[district_names_from_json[i]] = district_names_from_json[i] + '/' + district_ids_from_json[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_id_week = {}\n",
    "time_id_month = {}\n",
    "time_id_overall = {}\n",
    "date = datetime.date(2020,3,15)\n",
    "day = 1\n",
    "while True:\n",
    "    time_id_week[str(date)] = int(np.ceil(day/7))\n",
    "    time_id_month[str(date)] = int(str(date).split('-')[1])-2\n",
    "    time_id_overall[str(date)] = 1\n",
    "    if date == datetime.date(2020,9,5):\n",
    "        break\n",
    "    day+=1\n",
    "    date += datetime.timedelta(days=1)\n",
    "\n",
    "date = datetime.date(2020,3,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('https://api.covid19india.org/v4/data-all.json')\n",
    "dat = json.loads(r.text)\n",
    "\n",
    "date = datetime.date(2020,3,15)\n",
    "\n",
    "Date = []\n",
    "State = []\n",
    "District = []\n",
    "Dist_state = []\n",
    "numcases = []\n",
    "weekid = []\n",
    "monthid = []\n",
    "overallid = []\n",
    "pop = []\n",
    "\n",
    "while True:\n",
    "    for state in dat[str(date)]:\n",
    "        flag_districts=0\n",
    "        for key in dat[str(date)][state]:\n",
    "            if key == 'districts':\n",
    "                flag_districts=1\n",
    "                for district in dat[str(date)][state]['districts']:\n",
    "                    flag_delta=0\n",
    "                    for key_delta in dat[str(date)][state]['districts'][district]:\n",
    "                        if key_delta == 'delta':\n",
    "                            flag_delta=1\n",
    "                            flag_confimed=0\n",
    "                            for key_confirmed in dat[str(date)][state]['districts'][district]['delta']:\n",
    "                                if key_confirmed == 'confirmed':\n",
    "                                    flag_confirmed=1\n",
    "                                    Date.append(str(date))\n",
    "                                    State.append(state)\n",
    "                                    District.append(district)\n",
    "                                    Dist_state.append(district+':'+state)\n",
    "                                    numcases.append(dat[str(date)][state]['districts'][district]['delta']['confirmed'])\n",
    "                                    weekid.append(time_id_week[str(date)])\n",
    "                                    monthid.append(time_id_month[str(date)])\n",
    "                                    overallid.append(time_id_overall[str(date)])\n",
    "                                    if 'meta' in dat[str(date)][state]['districts'][district].keys():\n",
    "                                        if 'population' in dat[str(date)][state]['districts'][district]['meta'].keys():\n",
    "                                            pop.append(dat[str(date)][state]['districts'][district]['meta']['population'])\n",
    "                                        else:\n",
    "                                            pop.append(0)\n",
    "                                    else:\n",
    "                                        pop.append(0)\n",
    "        if flag_districts == 0:\n",
    "            flag_delta=0\n",
    "            if state== 'TT':\n",
    "                continue\n",
    "            for key in dat[str(date)][state]:\n",
    "                if key == 'delta':\n",
    "                    falg_delta=1\n",
    "                    flag_confirmed = 0\n",
    "                    for key_confirmed in dat[str(date)][state]['delta']:\n",
    "                        if key_confirmed == 'confirmed':\n",
    "                            flag_confirmed = 1\n",
    "                            Date.append(str(date))\n",
    "                            State.append(state)\n",
    "                            District.append('All Districts')\n",
    "                            numcases.append(dat[str(date)][state]['delta']['confirmed'])\n",
    "                            weekid.append(time_id_week[str(date)])\n",
    "                            monthid.append(time_id_month[str(date)])\n",
    "                            overallid.append(time_id_overall[str(date)])\n",
    "                            Dist_state.append('All Districts:'+state)\n",
    "                            pop.append(0)\n",
    "        \n",
    "    if date == datetime.date(2020,9,5):\n",
    "        break\n",
    "    date += datetime.timedelta(days=1)\n",
    "\n",
    "District_wise_data = {'Date': Date,'State': State, 'District': District, 'District_state': Dist_state, 'cases': numcases, 'weekid': weekid, 'monthid': monthid, 'overallid': overallid, 'Population': pop}\n",
    "District_wise_data = pd.DataFrame(District_wise_data)\n",
    "District_wise_data.sort_values(\"Population\", axis = 0, ascending = False, kind='mergesort', inplace = True, na_position ='last')\n",
    "District_wise_data.sort_values(\"State\", axis = 0, ascending = True, kind='mergesort', inplace = True, na_position ='last')\n",
    "District_wise_data = District_wise_data.reset_index(drop=True)\n",
    "# District_wise_data.to_csv('District_wise_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "populous_district = {}\n",
    "i=0\n",
    "while i<len(District_wise_data):\n",
    "    curr = District_wise_data['State'][i]\n",
    "    populous_district[District_wise_data['State'][i]] = District_wise_data['District'][i]\n",
    "    i+=1\n",
    "    while i<len(District_wise_data) and District_wise_data['State'][i] ==curr:\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('https://api.covid19india.org/v4/data-all.json')\n",
    "dat = json.loads(r.text)\n",
    "\n",
    "date = datetime.date(2020,3,15)\n",
    "\n",
    "Date = []\n",
    "State = []\n",
    "District = []\n",
    "Dist_state = []\n",
    "numcases = []\n",
    "weekid = []\n",
    "monthid = []\n",
    "overallid = []\n",
    "pop = []\n",
    "\n",
    "while True:\n",
    "    for state in dat[str(date)]:\n",
    "        flag_districts=0\n",
    "        for key in dat[str(date)][state]:\n",
    "            if key == 'districts':\n",
    "                flag_districts=1\n",
    "                for district in dat[str(date)][state]['districts']:\n",
    "                    flag_delta=0\n",
    "                    for key_delta in dat[str(date)][state]['districts'][district]:\n",
    "                        if key_delta == 'delta':\n",
    "                            flag_delta=1\n",
    "                            flag_confimed=0\n",
    "                            for key_confirmed in dat[str(date)][state]['districts'][district]['delta']:\n",
    "                                if key_confirmed == 'confirmed':\n",
    "                                    flag_confirmed=1 \n",
    "                                    Date.append(str(date))\n",
    "                                    State.append(state)\n",
    "                                    District.append(district)\n",
    "                                    Dist_state.append(district+':'+state)\n",
    "                                    numcases.append(dat[str(date)][state]['districts'][district]['delta']['confirmed'])\n",
    "                                    weekid.append(time_id_week[str(date)])\n",
    "                                    monthid.append(time_id_month[str(date)])\n",
    "                                    overallid.append(time_id_overall[str(date)])\n",
    "#                                     Dist_id.append(Dict[districts_to_complete_ids[district.lower()]])\n",
    "                                    if 'meta' in dat[str(date)][state]['districts'][district].keys():\n",
    "                                        if 'population' in dat[str(date)][state]['districts'][district]['meta'].keys():\n",
    "                                            pop.append(dat[str(date)][state]['districts'][district]['meta']['population'])\n",
    "                                        else:\n",
    "                                            pop.append(0)\n",
    "                                    else:\n",
    "                                        pop.append(0)\n",
    "        if flag_districts == 0:\n",
    "            flag_delta=0\n",
    "            if state== 'TT':\n",
    "                continue\n",
    "            for key in dat[str(date)][state]:\n",
    "                if key == 'delta':\n",
    "                    falg_delta=1\n",
    "                    flag_confirmed = 0\n",
    "                    for key_confirmed in dat[str(date)][state]['delta']:\n",
    "                        if key_confirmed == 'confirmed':\n",
    "                            flag_confirmed = 1\n",
    "                            Date.append(str(date))\n",
    "                            State.append(state)\n",
    "                            District.append(populous_district[state])\n",
    "                            numcases.append(dat[str(date)][state]['delta']['confirmed'])\n",
    "                            weekid.append(time_id_week[str(date)])\n",
    "                            monthid.append(time_id_month[str(date)])\n",
    "                            overallid.append(time_id_overall[str(date)])\n",
    "                            Dist_state.append(populous_district[state]+':'+state)\n",
    "#                             Dist_id.append(Dict[districts_to_complete_ids[populous_district[state].lower()]])\n",
    "                            pop.append(0)\n",
    "        \n",
    "    if date == datetime.date(2020,9,5):\n",
    "        break\n",
    "    date += datetime.timedelta(days=1)\n",
    "\n",
    "District_wise_data = {'Date': Date,'State': State, 'District': District, 'District_state': Dist_state, 'cases': numcases, 'weekid': weekid, 'monthid': monthid, 'overallid': overallid, 'Population': pop, 'ID': np.zeros(len(numcases))}\n",
    "District_wise_data = pd.DataFrame(District_wise_data)\n",
    "# District_wise_data.to_csv('District_wise_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neilrs/.local/lib/python3.6/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  del sys.path[0]\n",
      "/home/neilrs/.local/lib/python3.6/site-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "/home/neilrs/.local/lib/python3.6/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/home/neilrs/.local/lib/python3.6/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n",
      "/home/neilrs/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/neilrs/.local/lib/python3.6/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "/home/neilrs/.local/lib/python3.6/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(District_wise_data)):\n",
    "    if District_wise_data['District'][i].lower()=='hamirpur' and District_wise_data['State'][i] == 'HP':\n",
    "        District_wise_data['ID'][i] = Dict['hamirpur/316']\n",
    "    elif District_wise_data['District'][i].lower()=='bilaspur' and District_wise_data['State'][i] == 'CT':\n",
    "        District_wise_data['ID'][i] = Dict['bilaspur/190']\n",
    "    elif District_wise_data['District'][i].lower()=='pratapgarh' and District_wise_data['State'][i] == 'UP':\n",
    "        District_wise_data['ID'][i] = Dict['pratapgarh/546']\n",
    "    elif District_wise_data['District'][i].lower()=='balrampur' and District_wise_data['State'][i] == 'CT':\n",
    "        District_wise_data['ID'][i] = Dict['balrampur/148']\n",
    "    elif District_wise_data['District'][i].lower()=='aurangabad' and District_wise_data['State'][i] == 'BR':\n",
    "        District_wise_data['ID'][i] = Dict['aurangabad/133']\n",
    "    elif District_wise_data['District'][i].lower() in districts_to_complete_ids.keys():\n",
    "        District_wise_data['ID'][i] = Dict[districts_to_complete_ids[District_wise_data['District'][i].lower()]]\n",
    "    else:\n",
    "        District_wise_data['ID'][i] = 0\n",
    "\n",
    "indexNames = District_wise_data[District_wise_data['ID'] == 0].index\n",
    "District_wise_data.drop(indexNames , inplace=True)\n",
    "District_wise_data = District_wise_data.reset_index(drop=True)\n",
    "\n",
    "# District_wise_data.to_csv('District_wise_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neilrs/.local/lib/python3.6/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "state_data = {'districtid': District_wise_data['ID'], 'State': District_wise_data['State']}\n",
    "state_data = pd.DataFrame(state_data)\n",
    "\n",
    "sorted1 = state_data.sort_values(\"districtid\", axis = 0, ascending = True, kind='mergesort', inplace = False, na_position ='last')\n",
    "sorted2 = sorted1.sort_values(\"State\", axis = 0, ascending = True, kind='mergesort', inplace = False, na_position ='last')\n",
    "sorted2['dropped'] = 0\n",
    "sorted2 = sorted2.reset_index(drop=True)\n",
    "for i in range(len(sorted2)-1):\n",
    "    if sorted2['districtid'][i] == sorted2['districtid'][i+1]:\n",
    "        sorted2['dropped'][i] = 1\n",
    "indexNames = sorted2[ sorted2['dropped'] == 1].index\n",
    "sorted2.drop(indexNames , inplace=True)\n",
    "sorted2 = sorted2.drop(['dropped'], axis = 1)\n",
    "sorted2 = sorted2.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_districts = {}\n",
    "for i in range(len(sorted2)):\n",
    "    if sorted2['State'][i] in state_districts.keys():\n",
    "        state_districts[sorted2['State'][i]].append(int(sorted2['districtid'][i]))\n",
    "    else:\n",
    "        state_districts[sorted2['State'][i]] = []\n",
    "        state_districts[sorted2['State'][i]].append(int(sorted2['districtid'][i]))\n",
    "\n",
    "state_map = {}\n",
    "for key in state_districts.keys():\n",
    "    for i in range(len(state_districts[key])):\n",
    "        state_map[state_districts[key][i]] = state_districts[key].copy()\n",
    "\n",
    "for key in state_map.keys():\n",
    "    state_map[key].remove(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# week\n",
    "cases_week = pd.read_csv('cases-week.csv')\n",
    "cases_week = cases_week.drop(['Unnamed: 0'], axis = 1)\n",
    "\n",
    "weeks = int(cases_week['weekid'].max())\n",
    "\n",
    "Data1 = {'districtid': [],'weekid': [],'statemean': [], 'statestdev': [] }\n",
    "state_week = pd.DataFrame(Data1)\n",
    "\n",
    "for i in range(1,weeks+1):\n",
    "    start = len(state_map)*(i-1)\n",
    "    end = len(state_map)*i\n",
    "    numcases = {}\n",
    "    for j in range(start,end):\n",
    "        numcases[int(cases_week['districtid'][j])] = int(cases_week['cases'][j])\n",
    "    statemean = []\n",
    "    statestdev = []\n",
    "    for j in range(101,101+len(state_map)):\n",
    "        statecases = []\n",
    "        for k in range(len(state_map[j])):\n",
    "            statecases.append(numcases[state_map[j][k]])\n",
    "        statecases = np.array(statecases)\n",
    "        if len(statecases)>0:\n",
    "            statemean.append(np.mean(statecases))\n",
    "            statestdev.append(np.std(statecases))\n",
    "        else:\n",
    "            statemean.append(0)\n",
    "            statestdev.append(0)\n",
    "\n",
    "        \n",
    "    Data1 = {'districtid': np.arange(101,101+len(districts_from_json)),'weekid': i*np.ones(len(districts_from_json)),'statemean': statemean, 'statestdev': statestdev }\n",
    "    Data1 = pd.DataFrame(Data1)\n",
    "    state_week = state_week.append(Data1)\n",
    "\n",
    "state_week = state_week.reset_index(drop=True)\n",
    "state_week.to_csv('state-week.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# month\n",
    "cases_month = pd.read_csv('cases-month.csv')\n",
    "cases_month = cases_month.drop(['Unnamed: 0'], axis = 1)\n",
    "\n",
    "months = int(cases_month['monthid'].max())\n",
    "\n",
    "Data1 = {'districtid': [],'monthid': [],'statemean': [], 'statestdev': [] }\n",
    "state_month = pd.DataFrame(Data1)\n",
    "\n",
    "for i in range(1,months+1):\n",
    "    start = len(state_map)*(i-1)\n",
    "    end = len(state_map)*i\n",
    "    numcases = {}\n",
    "    for j in range(start,end):\n",
    "        numcases[int(cases_month['districtid'][j])] = int(cases_month['cases'][j])\n",
    "    statemean = []\n",
    "    statestdev = []\n",
    "    for j in range(101,101+len(state_map)):\n",
    "        statecases = []\n",
    "        for k in range(len(state_map[j])):\n",
    "            statecases.append(numcases[state_map[j][k]])\n",
    "        statecases = np.array(statecases)\n",
    "        if len(statecases)>0:\n",
    "            statemean.append(np.mean(statecases))\n",
    "            statestdev.append(np.std(statecases))\n",
    "        else:\n",
    "            statemean.append(0)\n",
    "            statestdev.append(0)\n",
    "        \n",
    "    Data1 = {'districtid': np.arange(101,101+len(districts_from_json)),'monthid': i*np.ones(len(districts_from_json)),'statemean': statemean, 'statestdev': statestdev }\n",
    "    Data1 = pd.DataFrame(Data1)\n",
    "    state_month = state_month.append(Data1)\n",
    "\n",
    "state_month = state_month.reset_index(drop=True)\n",
    "state_month.to_csv('state-month.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overall\n",
    "cases_overall = pd.read_csv('cases-overall.csv')\n",
    "cases_overall = cases_overall.drop(['Unnamed: 0'], axis = 1)\n",
    "\n",
    "overalls = int(cases_overall['overallid'].max())\n",
    "\n",
    "Data1 = {'districtid': [],'overallid': [],'statemean': [], 'statestdev': [] }\n",
    "state_overall = pd.DataFrame(Data1)\n",
    "\n",
    "for i in range(1,overalls+1):\n",
    "    start = len(state_map)*(i-1)\n",
    "    end = len(state_map)*i\n",
    "    numcases = {}\n",
    "    for j in range(start,end):\n",
    "        numcases[int(cases_overall['districtid'][j])] = int(cases_overall['cases'][j])\n",
    "    statemean = []\n",
    "    statestdev = []\n",
    "    for j in range(101,101+len(state_map)):\n",
    "        statecases = []\n",
    "        for k in range(len(state_map[j])):\n",
    "            statecases.append(numcases[state_map[j][k]])\n",
    "        statecases = np.array(statecases)\n",
    "        if len(statecases)>0:\n",
    "            statemean.append(np.mean(statecases))\n",
    "            statestdev.append(np.std(statecases))\n",
    "        else:\n",
    "            statemean.append(0)\n",
    "            statestdev.append(0)\n",
    "        \n",
    "    Data1 = {'districtid': np.arange(101,101+len(districts_from_json)),'overallid': i*np.ones(len(districts_from_json)),'statemean': statemean, 'statestdev': statestdev }\n",
    "    Data1 = pd.DataFrame(Data1)\n",
    "    state_overall = state_overall.append(Data1)\n",
    "\n",
    "state_overall = state_overall.reset_index(drop=True)\n",
    "state_overall.to_csv('state-overall.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
